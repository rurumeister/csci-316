{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GROUP ASSIGNMENT 1**\n",
    "\n",
    "| Name      | UOW ID |\n",
    "| ----------- | ----------- |\n",
    "| Calaunan Alexander Jr Sumampong      | 7559161       |\n",
    "| Deon Cham Hui Ern   | 7559471        |\n",
    "| Elroy Chua Ming Xuan | 7431673 |\n",
    "| Gonzales Raizel Vera Marie L. | 7436634 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover and Visualise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data file\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Be able to see all the columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# see the first 20 columns\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check datatype of all cols in df\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistics of the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all columns that have missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @raizel TO-DO data visualization, perform the following:\n",
    "### 1. Plot the distribution of the target variable and each of the features \n",
    "### 2. Plot the correlation matrix \n",
    "### 3. Plot the boxplots of the target variable and each of the features\n",
    "### 4. Plot the scatterplots of the target variable versus each of the features \n",
    "## ^ all suggestions, you can pick and choose depending on what you think is important and edit as you see fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pointbiserialr\n",
    "import scipy.stats\n",
    "\n",
    "# List of numerical features for point-biserial correlation\n",
    "numerical_features = [\n",
    "    'loan_amnt', 'funded_amnt_inv', 'term', 'int_rate', 'installment', 'emp_length',\n",
    "    'annual_inc', 'dti', 'inq_last_6mths', 'open_acc', 'pub_rec', 'revol_bal',\n",
    "    'revol_util', 'total_acc', 'out_prncp', 'out_prncp_inv', 'total_pymnt',\n",
    "    'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee',\n",
    "    'recoveries', 'collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt',\n",
    "    'next_pymnt_d', 'last_credit_pull_d', 'collections_12_mths_ex_med',\n",
    "    'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim'\n",
    "]\n",
    "\n",
    "# List of categorical features for Cramer's V correlation\n",
    "categorical_features = [\n",
    "    'grade', 'sub_grade', 'emp_length', 'verification_status', 'pymnt_plan', 'purpose',\n",
    "    'delinq_2yrs', 'initial_list_status', 'policy_code', 'application_type'\n",
    "]\n",
    "\n",
    "# # Compute Cramer's V correlations with the target attribute for categorical features\n",
    "# Define Cramer's V function\n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = scipy.stats.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
    "    rcorr = r - ((r - 1) ** 2) / (n - 1)\n",
    "    kcorr = k - ((k - 1) ** 2) / (n - 1)\n",
    "    return np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))\n",
    "\n",
    "# Compute Cramer's V for all the categorical features\n",
    "cramers_v_features = pd.Series(index=categorical_features)\n",
    "for column in categorical_features:\n",
    "    cramers_v_features[column] = cramers_v(df[column], df['default_ind'])\n",
    "\n",
    "# Compute point-biserial correlations with the target attribute for numerical features\n",
    "point_biserial_correlations = pd.Series(index=numerical_features)\n",
    "for column in numerical_features:\n",
    "    point_biserial_correlations[column] = pointbiserialr(df[column], df['default_ind'])[0]\n",
    "\n",
    "# Sort the correlations in descending order\n",
    "cramers_v_features_sorted = cramers_v_features.sort_values(ascending=False)\n",
    "point_biserial_correlations_sorted = point_biserial_correlations.sort_values(ascending=False)\n",
    "\n",
    "# Display the sorted results\n",
    "print(\"\\nPoint-Biserial Correlations (sorted):\")\n",
    "print(point_biserial_correlations_sorted)\n",
    "\n",
    "# Display the sorted results\n",
    "print(\"\\nCramer's V Correlation (sorted):\")\n",
    "print(cramers_v_features_sorted)\n",
    "\n",
    "# Combine the results into a single DataFrame\n",
    "all_correlations = pd.concat([point_biserial_correlations_sorted, cramers_v_features_sorted])\n",
    "all_correlations = all_correlations.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nAll Correlations (sorted):\")\n",
    "print(all_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations Visualised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HEATMAP OF CORRELATIONS\n",
    "# Step 1: Create a new DataFrame with the combined correlation values\n",
    "correlation_df = pd.DataFrame({'correlation': all_correlations})\n",
    "\n",
    "# Step 2: Create a Seaborn heatmap to visualize the correlation matrix\n",
    "plt.figure(figsize=(1, 12))\n",
    "sns.heatmap(correlation_df, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Correlation Heatmap with Default_ind')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin stuff before we start\n",
    "- Change the values of 'term' column into int type\n",
    "- Change all the dates from str to datetime type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the data types for columns with mixed types\n",
    "dtype_dict = {'pymnt_plan': 'str', 'last_pymnt_amnt': 'float', 'dti_joint': 'float'}\n",
    "\n",
    "# Import the CSV file with specified data types\n",
    "df = pd.read_csv('data.csv', dtype=dtype_dict)\n",
    "\n",
    "# Change the values of 'term' into integers\n",
    "df['term'] = df['term'].str.replace('months', '').astype(int)\n",
    "\n",
    "# Chnage the type of 'issue_d' to datetime\n",
    "df['issue_d'] = pd.to_datetime(df['issue_d'])\n",
    "df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'])\n",
    "df['last_pymnt_d'] = pd.to_datetime(df['last_pymnt_d'])\n",
    "df['next_pymnt_d'] = pd.to_datetime(df['next_pymnt_d'])\n",
    "df['last_credit_pull_d'] = pd.to_datetime(df['last_credit_pull_d'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline \n",
    "\n",
    "- Drop ID columns (id, member_id, emp_title, title, zip_code, desc) \n",
    "- Deal with missing data \n",
    "    - Remove mostly-null columns (cols with >50% missing data)\n",
    "    - Median impute (via SimpleImputer)\n",
    "- OneHotEncode categorical (nominal) columns; ordinal for categorical (ordinal) data  \n",
    "- Scale data (via StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.num_imputer = SimpleImputer(strategy=\"median\")\n",
    "        self.cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "        self.oneHot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "        self.ordinal_encoder = OrdinalEncoder()\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "        # Categorical columns\n",
    "        self.cat_cols = None\n",
    "        self.ord_cols = None\n",
    "\n",
    "        # Numerical columns\n",
    "        self.num_cols = None\n",
    "\n",
    "    def drop_rdnt_columns(self, X):\n",
    "        # Drop redundant columns\n",
    "        return X.drop(['zid', 'member_id', 'emp_title', 'title', 'zip_code', 'desc'], axis=1)\n",
    "    \n",
    "    def drop_mostly_emp_columns(self, X):\n",
    "        # Drop columns with >50% missing values\n",
    "        cols_to_drop = X.columns[X.isnull().mean() > 0.5]\n",
    "        return X.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Find categorical and numeric columns\n",
    "        self.cat_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "        self.num_cols = X.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "        # List of ordinal columns\n",
    "        self.ord_cols = ['emp_length', 'grade', 'sub_grade']\n",
    "\n",
    "        # Remove ordinal columns from categorical columns\n",
    "        self.cat_cols = [col for col in self.cat_cols if col not in self.ord_cols]\n",
    "\n",
    "        # print categorical and numeric columns\n",
    "        print(\"Categorical (nominal) columns: \", self.cat_cols)\n",
    "        print(\"Numeric columns: \", self.num_cols)\n",
    "        \n",
    "        # Fill missing values for numerical columns\n",
    "        X[self.num_cols] = self.num_imputer.fit_transform(X[self.num_cols])\n",
    "        # Fill missing values for ordinal columns\n",
    "        X[self.ord_cols] = self.cat_imputer.fit_transform(X[self.ord_cols])\n",
    "        # Fill missing values for categorical (nominal) columns\n",
    "        X[self.cat_cols] = self.cat_imputer.fit_transform(X[self.cat_cols])\n",
    "\n",
    "        # fit the encoder for nominal columns\n",
    "        self.oneHot_encoder.fit(X[self.cat_cols])\n",
    "        # fit the ordinal encoder\n",
    "        self.ordinal_encoder.fit(X[self.ord_cols])\n",
    "        # Fit the StandardScaler\n",
    "        self.scaler.fit(X[self.num_cols])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # Impute missing data in numeric columns\n",
    "        X[self.num_cols] = self.num_imputer.transform(X[self.num_cols])\n",
    "        \n",
    "        # One-hot encode categorical columns\n",
    "        categorical_data = self.oneHot_encoder.transform(X[self.cat_cols]).toarray()\n",
    "        X = X.drop(columns=self.cat_cols)\n",
    "        X = pd.concat([X, pd.DataFrame(categorical_data, columns=self.oneHot_encoder.get_feature_names_out(self.cat_cols))], axis=1)\n",
    "        \n",
    "        # Ordinal encode ordinal columns\n",
    "        X[self.ord_cols] = self.ordinal_encoder.transform(X[self.ord_cols])\n",
    "\n",
    "        # Scale numeric data\n",
    "        X[self.num_cols] = self.scaler.transform(X[self.num_cols])\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        # Drop columns and remove mostly-null columns before fitting and transforming \n",
    "        X = self.drop_rdnt_columns(X)\n",
    "        self.drop_mostly_emp_columns(X)\n",
    "        \n",
    "        return self.fit(X, y).transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming data with a custom transformer\n",
    "\n",
    "# Instantiate and use the CustomTransformer\n",
    "transformer = CustomTransformer()\n",
    "\n",
    "X = df.drop(columns=['default_ind'])  # Features\n",
    "y = df['default_ind']  # Target column\n",
    "\n",
    "# Fit and transform the data\n",
    "df = transformer.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the head of the processed data\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values of the processed data \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features (X) and the target column (y)\n",
    "X = df.drop(columns=['default_ind'])  # Features\n",
    "y = df['default_ind']  # Target column\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the train and test sets\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select and Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
